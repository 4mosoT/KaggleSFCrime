---
title: "SF Crime"
author: "Marco Formoso"
output: pdf_document
geometry: margin=0.5in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Introducción
Se ha elegido para este trabajo la competición de Kaggle de la evolución del crimen en San Francisco, se puede acceder en el siguiente link: <https://www.kaggle.com/c/sf-crime>. Es una opción interesante ya que podemos ver y hacer gráficos sobre el mapa de San Franciso directamente como se verá a continuación.

Primero cargamos las librerías necesarias:
```{r, message=FALSE}
library(plyr)
library(dplyr)
library(ggmap)
library(ggplot2)
library(readr)
library(lubridate)
library(caret)
library(knitr)
library(stringr)
```
Tenemos opción de usar librería **doMC** para poder hacer multiprocesamiento cuando entrenemos los modelos:
```{r, eval=FALSE}
library(doMC)
registerDoMC(cores = 4)
```

Cargamos los datos:
```{r}
train <- read_csv("./input/train.csv.zip")


```

En la página de Kaggle nos dice que las variables **Resolution** y **Descript** solo existen en el conjunto de entreno proporcionado. Optamos por suprimirlas también en nuestro conjunto de entreno.
```{r}
train$Resolution <- NULL
train$Descript <- NULL
```
\newpage
Damos un primer vistazo a los datos de entreno, comprobando así que se han cargado satisfactoriamente y vemos que pinta tiene:
```{r}
options(dplyr.width = Inf)
kable(head(train))
summary(train)
```

Vemos también que tipo de delitos manejaremos y cuantos hay de cada uno:
```{r}
sort(table(train$Category), decreasing = TRUE)
```

Es conveniente que separemos las fecha en horas, días, mes y año, ya que aunque se creen nuevas variables, es una mejor forma de tratarlo y podemos pensar que ciertos delitos pueden darse más en unas horas que otras. Por ejemplo, sguramente es más probable un robo de noche que de día.

```{r}
train <- mutate(train,
                Year = factor(year(Dates), levels = 2003:2015), 
                Month = factor(month(Dates), levels = 1:12), 
                Day = factor(day(Dates), levels = 1:31),
                Hour = factor(hour(Dates), levels = 0:23),
                DayOfWeek = factor(DayOfWeek, levels=c("Monday",
                                                  "Tuesday",
                                                  "Wednesday",
                                                  "Thursday",
                                                  "Friday",
                                                  "Saturday",
                                                  "Sunday"))
                )
train$Dates <- NULL



```


Acortamos los nombres de las calles, así solo nos quedamos con el nombre de la calle, no el número de bloque, casa, etc...:
```{r}
train$ShortAddr <- word(train$Address, start=-2, end=-1)
kable(head(train[,-6:-1]))
```

\newpage
#Visualización


Vemos los histogramas de las distintas variables para ver si encontramos alguna interesante. Podemos pensar que por ejemplo el barrio influye mucho, así como el día de la semana o la hora que sea.
```{r, fig.pos = 'p', fig.show='hold', out.width='50%'}
ggplot(train, aes(x=PdDistrict)) + geom_bar(colour='black', fill='skyblue')+ggtitle('# crimenes por distrito')
ggplot(train, aes(x=DayOfWeek)) + geom_bar(colour='black', fill='skyblue')+ggtitle('# crimenes por dia semana')
ggplot(train, aes(x=Hour)) + geom_bar(colour='black', fill='skyblue')+ggtitle('# crimenes por hora')

```

Podemos conseguir un mapa de San Franciso con la librería ggmap y poder dónde se encuentran los distintos distritos.
```{r, fig.align='center', message=FALSE, warning=FALSE, eval=FALSE}
map<-get_map(location="sanfrancisco", zoom= 12, color = "bw")

ggmap(map) +
     geom_point(data=train[1:200000,], aes(x=X, y=Y, color=factor(PdDistrict)), alpha=0.05) +
     guides(colour = guide_legend(override.aes = list(alpha=1.0, size=6.0),
                                  title="PdDistrict")) +
     scale_colour_brewer(type="qual",palette="Paired") +
     ggtitle("Map of PdDistricts")

```
![](sf_districts_map.png)

\newpage


```{r}

dummies <- dummyVars( ~ Hour + DayOfWeek, data = train)
dummy_train <- data.frame(predict(dummies, newdata= train))
dummy_train$Category <- train$Category
dummy_train$X <- train$X
dummy_train$Y <- train$Y
train <- dummy_train
rm(dummy_train)

```

\newpage

```{r, echo = FALSE}
load('models/model_c50.Rdata')
load('models/model_knn.Rdata')
load('models/model_rf.Rdata')


```



#Construcción del modelo

Se han escogido los siguientes algoritmos de clasificación: Knn, C5.0 y random forest.

* Knn: El algoritmo de "K vecinos cercanos"" se basa en calcular la distancia entre vectores dados, uno es el elemento a clasificar y los otro K, los vecinos más cercanos. 
* C5.0: Es una modificación del árbol de decisión C4.5. Algunas de las mejoras es la velocidad y el uso mas eficiente de memoria, asi como obtener árboles de decisión más pequeños. C4.5 construye un árbol de decisión, usando el concepto de entropía de información.
* Random Forest: Es un algoritmo de bagging, en el que se promedian varios árboles de decisión y se van escogiendo distintos atributos al azar para obetener la clasificación final.


```{r, eval = FALSE}
train$Category <- make.names(train$Category)

train_partition <- createDataPartition(y=train$Category, p=.1, list=FALSE)
training <- train[train_partition,]
test <- train[-train_partition,]

ctrl <- trainControl(method = "repeatedcv",number=5, repeats=3,classProbs=TRUE, summaryFunction=mnLogLoss)
formula <- Category ~ .
```
Aquí simplemente dividimos la partición de datos en train y test. Para evaluar los distintos modelos vamos a usar el 10% de los datos para training por la limitaciones de hardware, ya que tenemos cerca de un millón de instancias.\newline
En traincontrol fijamos las opciones de validación. En este caso un 5 validación cruzada con 3 repeticiones debido al alto número de instancias. \newline
Para poder evaluar nuestro modelo en Kaggle, debemos de proporcionar las probabilidades de pertenecer a cualquier clase por eso se hace necesaria la inclusión de "classProbs=TRUE", como así también la métrica a usar que en este caso es la pérdida logarítmica.




```{r, eval = FALSE}

knngrid <- expand.grid(kmax=c(3,5,7,9,15,20), distance=c(1,2), kernel=c('gaussian', 'optimal', 'inv'))
model_knn <- train (formula, tuneGrid = knngrid, data=training,method='kknn',trControl=ctrl, 
                    metric="logLoss", verbose = TRUE)

model_rf <- train (formula, tuneLength=10, data=training,method='rf',trControl=ctrl, 
                   metric="logLoss", verbose = TRUE)
model_c50 <- train (formula, tuneLength=10, data=training,method='C5.0',trControl=ctrl, 
                    metric="logLoss", verbose = TRUE)

```
En estas líneas entrenamos el modelo.\newline
Hemos fijado los valores de tuneLength en 10, para que entrene modelos con distintos parámetros. Caret se encarga de combinar los distintos parámetros si se le puede proporcionar más de uno al algoritmo.
Particular atención recibe knngrid, en el que nosotros ajustamos los distintos parámetros a probar para el clasificador, ya que simplemente modificando tuneLength, 'caret' no nos proporciona distintas distancias o kernels.
 
 
```{r}
plot(model_knn)
```

Observamos en las gráficas como en Knn apenas hay diferencias en las distancias usadas, 1 correspondería a la distancia euclídea y 2 a la distancia manhattan. También observamos que usando un kernel 'optimal' obetenemos la mejor pérdida logarítmica y que parece que la tendencia es que al ir aumentando el número de vecinos disminuye la pérdida.

```{r}
plot(model_rf)
```


Para random forest parece ser que usando 8 predictores obtenemos el modelo óptimo.

```{r}
plot(model_c50)
```

En C50 con winnowing es cuando mejores predicciones obtenemos y usando un modelo tree.

```{r, fig.pos = 'p', fig.show='hold', out.width='50%'}
results <- resamples(list(KNN=model_knn,RF=model_rf, C50=model_c50))
summary(results)
bwplot(results)
dotplot(results)
```



```{r}
diffs<-diff(results)
summary(diffs)

```

Vemos claramente como el mejor algoritmo a usar en este caso sería C50, aunque si queremos una menor varianza deberíamos de usar Random Forest. En cualquier caso, si queremos ajustar más el resultado, habría que seguir trabajando con uno de estos dos algoritmos, descartando Knn. 






